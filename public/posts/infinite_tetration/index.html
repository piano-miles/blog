<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Efficiently Computing Infinite Tetration | Waugh Blog</title><meta name=keywords content="math,numerical analysis,long"><meta name=description content="I analyze the infinite tetration (or &ldquo;power tower&rdquo;) and derive efficient methods for computing it."><meta name=author content><link rel=canonical href=https://blog.mileswaugh.com/posts/infinite_tetration/><link crossorigin=anonymous href=/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=https://blog.mileswaugh.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.mileswaugh.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blog.mileswaugh.com/favicon-32x32.png><link rel=apple-touch-icon href=https://blog.mileswaugh.com/apple-touch-icon.png><link rel=mask-icon href=https://blog.mileswaugh.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://blog.mileswaugh.com/posts/infinite_tetration/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})</script><meta property="og:url" content="https://blog.mileswaugh.com/posts/infinite_tetration/"><meta property="og:site_name" content="Waugh Blog"><meta property="og:title" content="Efficiently Computing Infinite Tetration"><meta property="og:description" content="I analyze the infinite tetration (or “power tower”) and derive efficient methods for computing it."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-28T18:00:00-08:00"><meta property="article:modified_time" content="2025-03-28T18:00:00-08:00"><meta property="article:tag" content="Math"><meta property="article:tag" content="Numerical Analysis"><meta property="article:tag" content="Long"><meta name=twitter:card content="summary"><meta name=twitter:title content="Efficiently Computing Infinite Tetration"><meta name=twitter:description content="I analyze the infinite tetration (or &ldquo;power tower&rdquo;) and derive efficient methods for computing it."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.mileswaugh.com/posts/"},{"@type":"ListItem","position":2,"name":"Efficiently Computing Infinite Tetration","item":"https://blog.mileswaugh.com/posts/infinite_tetration/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Efficiently Computing Infinite Tetration","name":"Efficiently Computing Infinite Tetration","description":"I analyze the infinite tetration (or \u0026ldquo;power tower\u0026rdquo;) and derive efficient methods for computing it.","keywords":["math","numerical analysis","long"],"articleBody":"Introduction In this blog post, I derive two efficient methods to stably compute the infinite tetration (or “power tower”). “Tetration” simply means repeated exponentiation, so “infinite tetration” simply means exponentiation an infinite number of times. The infinite tetration of \\(x\\), which I will call \\(y\\), is defined as:\n$$y=x^{x^{x^{.^{.^{.}}}}}$$I will begin with a simple “naïve” approach to solving the problem. Then, I will more deeply analyze the structure of the problem. From there, I will discuss two approaches to solving the problem efficiently.\nThe first method I derive turns the problem into an integral which can be solved numerically. The second method that I derive, and the main focus of this blog post, is an iterative method that achieves stable quadratic convergence for all valid \\(x\\). “Iterative” simply means that the method constructs successively more accurate approximations until the approximation error is within a desired tolerance of the true solution.\nThe Naïve Approach The most straightforward way to compute the infinite power tower \\(y\\) of an input \\(x\\) is to repeatedly raise \\(x\\) to the power of more \\(x\\)’s. The idea is that the infinite power tower, which we can not compute (it requires an infinite number of operations), is sufficiently similar to a very large power tower, which we can compute. As we will discover later, however, there are certain inputs where this approach is problematic. It is important to note that repeated exponentiation is right-associative, not left-associative. That means \\(a^{b^{c}}=a^{\\left(b^{c}\\right)}\\neq\\left(a^{b}\\right)^{c}\\). Mathematically, the Naïve approach is then:\n$$y_0 = x$$ $$y_1 = x^x = x^{y_0}$$ $$y_2 = x^{x^{x}} = x^{y_1}$$ $$\\vdots$$ $$y_n = x^{y_{n-1}}$$In the limit as we raise \\(x\\) to the power of itself more times, it converges on the solution \\(y\\) if one exists. However, this approach converges quite slowly for many values of \\(x\\). Furthermore, for small \\(x\\), the sequence oscillates indefinitely, and the values between which it oscillates converge.\nTo see how inefficient this is for some inputs, look at this example of convergence of 0.4 raised to the power of itself repeatedly:\nn \\(y_n\\) Absolute error ( \\(\\left | y_n-y_{\\infty}\\right | \\) ) 0 0.40000000000 0.18504317197 1 0.69314484316 0.10810167118 2 0.52987073647 0.05517243550 3 0.61537979747 0.03033662550 4 0.56900457469 0.01603859728 5 0.59370446436 0.00866129238 … … … 35 0.58504317204 0.00000000007 36 0.58504317194 0.00000000003 37 0.58504317199 0.00000000002 38 0.58504317196 0.00000000001 39 0.58504317198 0.00000000001 Here is an example of “oscillatory” behavior with 0.02 raised to the power of itself repeatedly:\nn \\(y_n\\) 0 0.0200000000 1 0.9247420362 2 0.0268467067 3 0.9003020739 … … 26 0.0314614034 27 0.8841949279 28 0.0314614938 29 0.8841946152 30 0.0314615323 As we approach an infinite number of iterations, the sequence tends toward bouncing between two values. Of course, you might imagine that large numbers raised to the power of themselves will keep getting larger forever. Here is an example of 1.5 raised to itself repeatedly, where the infinite tetration spirals to infinity:\nn \\(y_n\\) 1 1.500000000 2 1.837117307 3 2.106203352 … … 9 4.382546732 10 5.911914873 11 10.99098293 12 86.18189174 13 1,499,263,005,586,587 14 5.400339825 × 10^264,007,110,309,345 By the 14th iteration, it is quite evident that the sequence tends to infinity.\nTo better understand these types of behaviors, I’ve plotted the true infinite tetration curve below, in green, alongside the naïve approach for computing it, in red:\nNotice how, for small values below a threshold between 0 and 0.2, the numbers oscillate between two values in the limit as the number of iterations approaches infinity. These two values are denoted by the dashed green line. In this region, the infinite tetration is technically not defined, because it does not converge to a single value. Above that “branching point,” until a point a bit greater than 1.4, the infinite tetration does converge to a single value. These are denoted by the solid green line. Finally, above some point between 1.4 and 1.5, the infinite tetration diverges toward infinity.\nGiven these observations, I would like to answer the following questions:\nWhen does the infinite tetration converge? When does it oscillate indefinitely? When does it blow up to infinity? Can we rigorously derive exactly where these transitions occur? Can we find a “better” way to compute the result of the infinite tetration (either what value the sequence converges to, or between what values it oscillates between)? Moving forward in this blog post, I will neglect the infinite tetration of negative numbers. This is because, inevitably, you will encounter complex numbers; I wish to remain within the reals this time. The only exception to this is the infinite tetration of -1, which is -1. And that’s rather boring.\nIn the next section, Structure Analysis, I will address the first bullet point. In the following two sections, Lambert W and Newton-Raphson, I will address the second bullet point.\nStructure Analysis Fixed-Point Iteration Recall the naïve approach:\n$$y_n = x^{y_{n-1}}$$We say that the sequence “converges” if the limit \\(y_{\\infty}=\\lim_{n\\to\\infty}y_n\\) exists. If the sequence does indeed converge, then the solution we seek, \\(y_{\\infty}\\), is unchanged when \\(x\\) is raised to its power (in both cases, \\(x\\) is raised to itself an infinite number of times):\n$$y_{\\infty}=x^{y_{\\infty}}$$Alternatively, we can think of the naïve approach as creating a variable \\(z\\) which is initialized to \\(x\\), and is updated by applying the function:\n$$z\\to f\\left(z\\right)$$Where \\(f\\left(z\\right)=x^z\\). If we repeatedly apply this, then we hope that \\(z\\) will approach \\(y_\\infty\\) if \\(y_{\\infty}\\) does exist. Our problem, then, is to find a “fixed point” of \\(f\\): that is, a value \\(z\\) such that \\(f\\left(z\\right)=z\\). This is called a fixed point because the value of \\(z\\) remains fixed under application of the function \\(f\\). Repeatedly passing the variable through the function and setting the variable to the resulting value is called a fixed-point iteration. Often, we can find fixed points of a function by simply repeatedly applying the fixed-point iteration. The variable will keep changing until it reaches the fixed point, after which it will remain at the fixed point.\nFor instance, take the function \\(f\\left(z\\right)=\\sin\\left(z\\right)+z\\). It is not difficult to realize that this function has an infinite number of fixed points at \\(z=n\\pi\\) for all \\(n\\in\\mathbb{Z}\\). At these points, \\(\\sin\\left(z\\right)\\) is 0, so the function simply returns \\(z\\). If we approximate the fixed point as \\(3\\), we can write some simple Python code and watch as the fixed-point iteration converges quickly on \\(\\pi\\):\nimport math z = 3.0 print(f\"z_0: {z:.11f}\") for i in range(4): z += math.sin(z) print(f\"z_{i+1}: {z:.11f}\") print(f\"π: {math.pi:.11f}\") Which gives the following output:\nz_0: 3.00000000000 z_1: 3.14112000806 z_2: 3.14159265357 z_3: 3.14159265359 z_4: 3.14159265359 π: 3.14159265359 Wow, it converged with just 3 iterations! In fact, this method converges with cubic convergence. I will not explain in detail what that means and why that is the case as it is out of the scope of this post, but the reason ends up being because \\(0=f’\\left(\\pi\\right)=f’’\\left(\\pi\\right)\\ne f^{\\left(3\\right)}\\left(\\pi\\right)=1\\). This method for computing pi is generally not very useful in practice, however, because the computer has to compute a Taylor series for the sine function, which involves a lot of floating-point operations per function call. We may as well just compute \\(\\pi=4\\arctan\\left(1\\right)\\) if we’re using trig functions at all, which is just a single function call.\nHopefully I’ve convinced you that fixed-point iteration can be very useful at times. However, we have no reason to believe that fixed-point iteration will always lead us to the fixed point of the function. Consider the function \\(f\\left(z\\right)=z^2-2\\) as an example. The fixed point \\(p\\) of the function \\(f\\) is easy to solve for:\n$$p=p^2-2$$ $$p^{2}-p-2=0$$ $$\\left(p-2\\right)\\left(p+1\\right)=0$$ $$p=2,-1$$Let’s say we estimate \\(p\\) to be about \\(z_0=1.5\\). It’s between the two fixed points, so surely it will converge on one. Right? We can write some simple Python code to perform the fixed-point iteration:\nz = 1.5 for _ in range(15): z = z * z - 2 print(f\"{z:+0.3f}\") This yields the following output:\n+0.250 -1.938 +1.754 +1.076 -0.842 -1.291 -0.332 -1.889 +1.570 +0.465 -1.783 +1.181 -0.606 -1.633 +0.668 It went all over the place! Using the fixed point iteration did not converge to any fixed points of the function. Maybe it would help if we provided an initial estimate that was very close to the value. Let us now begin with an initial guess very close to 2:\nz = 1.999 for _ in range(15): z = z * z - 2 print(f\"{z:+0.3f}\") But this does not converge to 2 either:\n+1.996 +1.984 +1.936 +1.749 +1.060 -0.876 -1.233 -0.479 -1.771 +1.135 -0.711 -1.494 +0.233 -1.946 +1.786 In fact, unless your initial estimate is exactly the fixed point, the fixed-point iteration will never converge on either of the fixed points! From this, it follows that we may want to distinguish between two types of fixed points: the type that we will converge on via the fixed-point iteration, and the type that we will never converge on.\nIt is finally time to characterize fixed points. If a fixed point has a neighborhood of points around it that converge on it, we call that fixed-point attracting. The largest such neighborhood of that point is called the basin of attraction. You might guess that the fixed point 2 in the second example is not attracting, whereas the fixed point \\(\\pi\\) from the first example is attracting.\nSimilarly, there is a notion of repelling fixed points, such as the fixed points in the second example. Even if your guess is very close to 2, the iteration will repel the guess away from 2. Finally, there is a notion of a periodic point, which is some value that the system will periodically return to. The points that the repeated tetration oscillates between in the limit as the iteration count approaches infinity are examples of periodic points. In this post, we will primarily be concerned with whether a fixed point is attracting or not, since that determines convergence. We will also be interested in finding an attracting fixed point’s basin of convergence, because that is a region within which an initial guess is theoretically guaranteed to converge.\nThis begs the question: is there a way to definitively prove whether a fixed point is attracting? Furthermore, can we determine its basin of attraction? This is exactly what the Banach–Caccioppoli fixed-point theorem (1922) allows us to do, provided that the fixed-point iteration \\(f\\) is continuous. Often, “Caccioppoli” is omitted when referencing the name of the theorem. While not strictly necessary, it also helps mathematically if the fixed-point iteration is differentiable. You’ll see why in a moment.\nI will briefly note, however, that the theorem also allows us to prove whether a fixed point exists in a region of the function’s domain, and whether that fixed point is the only (“unique”) fixed point in that region. This can be useful, for instance, if we can’t solve for the fixed points of a function. I will skip this part of the theorem entirely as it is not relevant to this blog post, but I may revisit it in a future post.\nGenerally, the function \\(f\\) is defined on a “metric space” \\(\\left(X,d\\right)\\). You can think of this as a set of points \\(X\\) equipped with the additional structure of a “distance,” \\(d\\), defined between points. \\(d\\) itself is a function that takes in two points from our space and outputs a nonnegative number, called the “distance” between the points. This distance function can be anything as long as it satisfies four simple axioms. I will briefly state them here in plain English, but they are not relevant to this post:\nThe distance from a point to itself is zero. The distance between two different points is positive. The distance from x to y is the distance from y to x. The distance between x and z can never be greater than the sum of distances between x and y and between y and z. The fixed-point iteration \\(f\\) must necessarily map \\(f: X\\to X\\) (it maps from a space onto the same space). This requirement should make intuitive sense: for a fixed point to be a fixed point, the function should not affect it. The fixed point \\(p\\) satisfies \\(f(p)=p\\), so \\(f(p)\\) and \\(p\\) are both in \\(X\\).\nWhat the Banach fixed-point theorem states is that a fixed point \\(p\\) of \\(f\\) will be attracting if \\(f\\) is a contraction mapping in some neighborhood around \\(p\\). Then, the largest such neighborhood is the basin of attraction. We say that \\(f\\) is a contraction mapping if there exists a constant \\(0\\leq k\u003c1\\) such that for all \\(x,y\\in X\\), the following inequality holds:\n$$d\\left(f(x),f(y)\\right)\\leq k\\cdot d(x,y)$$Intuitively, we choose two points in the space and apply the function to both. If the distances between the points contract under the fixed-point iteration for all pairs of points that we choose, then the function is a contraction mapping. It should make sense why this theorem is true: if the function application does not contract points into the fixed point, then points will never converge to the fixed point. But if the function application does contract points into the fixed point, then points will converge to the fixed point. I leave the proof of the theorem as an exercise to the reader, as it is a relatively straightforward proof.\nIn our case, because \\(f\\) is a map \\(f:\\mathbb{R}\\to\\mathbb{R}\\), we can use the Euclidean distance as our metric:\n$$d(x,y)=\\left|x-y\\right|$$The Banach fixed-point theorem states that our fixed-point iteration will be contractive over some domain \\(X\\subseteq\\mathbb{R}\\) if there exists a constant \\(0\\leq k\u003c1\\) such that, for all \\(x,y\\in X\\), the following inequality holds:\n$$\\left|f(x)-f(y)\\right|\\leq k\\cdot\\left|x-y\\right|$$Using the mean value theorem, we can rewrite this inequality as:\n$$\\left|f'(z)\\right|\\leq k$$Where \\(z\\) is some point between \\(x\\) and \\(y\\). This is a much more convenient form of the inequality to check. You can intuitively think of this as checking that the condition described is satisfied for the subset of all pairs of points that are infinitesimally close. If you wish to check the distance between two points that are a finite distance apart, then you must sum the distances between all the infinitesimally close points that lie between the two points. If all of the infinitesimal distances contract, then the finite distance which is their sum will also contract. If we can find a constant \\(0\\leq k\u003c1\\) such that the derivative of the fixed-point iteration is less than \\(k\\) for all points in a neighborhood of a fixed point, then we can conclude that the fixed-point attracting.\nWith my example of \\(f(z)=z^2-2\\), the fixed point 2 is not attracting because, where \\(f’(z)=2z\\):\n$$\\left|f'(2)\\right| = 4 \u003e 1 \u003e k$$The derivative is only less than 1 when \\(z\\in(-\\frac{1}{2},\\frac{1}{2})\\), which does not contain any fixed points of the function. Therefore, all fixed points of the function are not attracting, and the fixed-point iteration will not work.\nThis theorem will serve us well soon enough, since we observed that the infinite tetration can be represented as a fixed point of a function.\nPlotting Behavior Back to the infinite tetration: let’s make some plots. The first step is to express the infinite tetration \\(y\\) of \\(x\\) recursively as:\n$$x^y=y$$If we plot this equation, we get the following red curve. Ignore the green curve for now (we’ll talk about it shortly):\nInterestingly, if we instead graph the application of two iterations:\n$$x^{x^y}=y$$Then we get the same curve, but with an additional two branches when x is small, colored above as the green curve. These two green branches correspond to the values which the naïve sequence oscillates between in the limit as the number of iterations approaches infinity. When \\(x\\) is larger than where the green curve exists, the sequence will converge on the red curve. Finally, to the right of the point where the red curve bends backward (to the left, where there now appear to be multiple values for the tetration for a single input), which is also the upper bound on the domain of the curve, all values will diverge to infinity.\nWhy do the branches only appear with the more complicated expression? If you compute the iteration only once, then in the oscillatory region, the value will flip to the other value between which the sequence is oscillating. But if you apply the iteration twice, then the value will flip twice, back to its original value. The first graph plots what is invariant under a single iteration, while the second graph plots what is invariant under two iterations.\nI want to briefly also talk about the red-colored “middle” branch that lies between the two green branches. We observed before that repeatedly exponentiating \\(x\\) to its own power in this oscillatory region causes the value to oscillate between the two green branches. The middle branch is sort of a “ghost” solution—it satisfies the property that raising \\(x\\) to its values returns the same values (which is consistent with how we described the infinite tetration), but if we begin with \\(x\\) and repeatedly construct a larger and larger power tower, we will never reach this branch. Thus, I am not sure whether to call this a “valid solution” to the infinite tetration or not.\nDeriving Key Points Let’s establish some important points on the curve. The trivial cases are when \\(x=0\\) and \\(x=1\\). When \\(x=0\\), the infinite tetration oscillates between 0 and 1. This is because anything raised to the power of 0 is 1, and anything raised to the power of 1 is unaffected. So \\(0^0=1\\), \\(0^{0^0}=0^{1}=0\\), and the cycle repeats. Raising 1 to the power of itself is just 1, so the infinite tetration of 1 is 1.\nLet’s find out where the curve goes from converging to diverging. The curve is vertical when \\(\\frac{dx}{dy}=0\\). We can solve for \\(x\\): $$x^y=y\\implies x = \\sqrt[y]{y}$$ And differentiate: $$\\frac{dx}{dy}=-y^{\\frac{1}{y}-2}(\\ln(y)-1)=0$$ Which is true when \\(y^{\\frac{1}{y}-2}=0\\) and when \\(\\ln(y)=1\\). Thus, the curve is vertical when \\(y=0\\) and when \\(y=e\\). That means the curve is vertical when \\(x=0\\) and \\(x=\\sqrt[e]{e}\\). This tells us when the infinite tetration will diverge to infinity: whenever we are infinitely tetrating a number greater than \\(\\sqrt[e]{e}\\). The largest value that can be infinitely-tetrated is \\(\\sqrt[e]{e}\\), converging to the largest infinitely-tetrated value, \\(e\\).\nTo find when the curve transitions from oscillating to converging to a single value, we can use the Banach fixed-point theorem. The derivative of the fixed-point iteration \\(f\\left(y\\right)=x^{y}\\) is: $$f'\\left(y\\right)=\\frac{d}{dy}\\left[x^{y}\\right]=x^{y}\\ln\\left(x\\right)$$ Because the basin of attraction is where the magnitude of the derivative is less than 1, the boundary of the basin is when the derivative is exactly 1 or -1. If we consider \\(0 \u003c x \u003c 1\\), then \\(\\ln(x)\\) is strictly negative and \\(x^y\\) is strictly positive, meaning g’(y) is strictly negative. That means we can simply check when \\(g’(y) = -1\\). Thus, we are interested in the values of \\(x\\) such that: $$x^{y}\\ln\\left(x\\right)=-1\\implies y=\\frac{\\ln\\left(-\\frac{1}{\\ln\\left(x\\right)}\\right)}{\\ln\\left(x\\right)}$$ We also know, additionally, that \\(x^{y}=y\\), so: $$x^{\\frac{\\ln\\left(-\\frac{1}{\\ln\\left(x\\right)}\\right)}{\\ln\\left(x\\right)}}=\\frac{\\ln\\left(-\\frac{1}{\\ln\\left(x\\right)}\\right)}{\\ln\\left(x\\right)}$$ $$\\implies-\\frac{1}{\\ln(x)}=\\frac{\\ln\\left(-\\frac{1}{\\ln(x)}\\right)}{\\ln(x)}$$ $$\\implies \\frac{\\ln\\left(-\\frac{1}{\\ln(x)}\\right)+1}{\\ln(x)}=0$$ $$\\implies x=e^{-e}$$ And $$y=\\frac{\\ln\\left(-\\frac{1}{\\ln\\left(e^{-e}\\right)}\\right)}{\\ln\\left(e^{-e}\\right)}=\\frac{1}{e}$$Indeed, the curve is undefined on \\(\\left(-\\infty,0\\right)\\), oscillates on \\(\\left[0,e^{-e}\\right)\\), converges on \\(\\left[e^{-e},\\sqrt[e]{e}\\right]\\), and diverges on \\(\\left(\\sqrt[e]{e},\\infty\\right)\\).\nWhat we would like to do is formulate a method that efficiently converges to all branches in order to determine what the infinite tetration converges to, what it tends toward, or what it oscillates between.\nBefore we move onto the next section, I want to briefly provide very precise upper and lower bounds for what an input \\(x\\) could be given some infinite tetration \\(y\\) in the convergent region. I’ve obtained these via Padé approximation:\n$$\\frac{5+\\left(77y-40\\right)y}{13+\\left(\\left(4y+35\\right)y-10\\right)y}\\le x\\le3\\cdot\\frac{399+\\left(\\left(279y+3721\\right)y-2379\\right)y}{1072-\\left(133+\\left(\\left(89y-1709\\right)y-3501\\right)y\\right)y}$$If you are on a small device such as a phone, you can scroll to see the entire formula. These bounds are very accurate:\nThe infinite tetration curve is plotted in blue, the bounds are in orange, and the box of convergence is in green. The bounds are too accurate in the convergence box, so it is difficult to plot it. Here is only the convergence region:\nWhile I will not use them in this blog post, feel free to use them if you work on this subject any more.\nLambert W Our first step should be to attempt to solve for the infinite tetration, \\( y \\), directly. However, in order to solve for \\(y\\) exactly, we require the Lambert W function \\(W(z)\\), which can not be expressed using only elementary functions. The Lambert W function is the function that satisfies:\n$$W(z) e^{W(z)} = z$$You can get this result by rearranging \\(y=x^y\\) into\n$$\\ln\\left(\\frac{1}{y}\\right)e^{\\ln\\left(\\frac{1}{y}\\right)}=-\\ln\\left(x\\right)$$And, by the definition of the Lambert W function:\n$$W\\left(-\\ln\\left(x\\right)\\right)e^{W\\left(-\\ln\\left(x\\right)\\right)}=-\\ln\\left(x\\right)$$ $$\\implies W\\left(-\\ln\\left(x\\right)\\right)=\\ln\\left(\\frac{1}{y}\\right)$$From here, we can obtain the solution in terms of the Lambert W function:\n$$y=-\\frac{W\\left(-\\ln\\left(x\\right)\\right)}{\\ln\\left(x\\right)}$$Because we are left with the Lambert W function, which is irreducible, this motivates a numerical approach. The presence of that function demonstrates that it is impossible to exactly represent a formula in closed form. If our problem could be solved exactly using a finite number of elementary operations and functions, then so would the Lambert W function, which we know is not the case.\nOne potential technique to solve the problem is to compute the Lambert W function. We can employ an integral to compute the Lambert W function that was derived by Kalugin, Jeffrey, and Corless in 2011, giving:\n$$y=-\\frac{1}{\\ln\\left(x\\right)\\pi}\\int_{0}^{\\pi}\\ln\\left(1-\\frac{1}{t}\\ln\\left(x\\right)\\sin\\left(t\\right)e^{t\\cot\\left(t\\right)}\\right)dt$$Note that this integral gives only the center branch present in \\(y=x^y\\). The integrand is reasonably well-behaved and looks like this:\nAt the corner where \\(x=\\sqrt[e]{e}\\) and \\(t=0\\), the integrand diverges to \\(-\\infty\\). You can check this by substituting in the value for \\(x\\) and taking \\(t\\to0^{+}\\) in the limit to get:\n$$\\lim_{t\\to0^{+}}\\ln\\left(1-\\frac{1}{t}\\sin(t)e^{t\\cot(t)-1}\\right)$$We’re interested in the limit of the t-dependent term:\n$$\\lim_{t\\to0^{+}}\\frac{1}{t}\\sin(t)e^{t\\cot(t)-1}$$Notice that \\(\\sin\\left(x\\right)\\), \\(x\\), and \\(\\cot\\left(x\\right)\\) are all odd functions, so \\({\\sin\\left(x\\right)}/{x}\\) and \\(x\\cot\\left(x\\right)\\) are even, making the whole expression even. Thus, instead of only considering the right-hand limit, I will consider the limit from both sides (for simplicity, because the RH and LH limits equal each other).\nUsing L’Hôpital’s rule:\n$$\\lim_{t\\to0}\\frac{\\sin(2t)-t}{\\sin\\left(t\\right)}e^{t\\cot(t)-1}$$ $$=\\lim_{t\\to0}\\frac{\\sin(2t)-t}{\\sin\\left(t\\right)}\\cdot\\lim_{t\\to0}e^{t\\cot(t)-1}$$Using L’Hôpital’s rule again, the left-hand limit becomes:\n$$\\lim_{t\\to0}\\left[4 \\cos (t)-3 \\sec (t)\\right]=1$$Using L’Hôpital’s rule again on \\(t\\cot(t)={t}/{\\tan\\left(t\\right)}\\), the right-hand limit becomes:\n$$\\lim_{t\\to0}e^{\\cos^{2}\\left(t\\right)-1}=1$$If you differentiate the original term \\(\\frac{1}{t}\\sin(t)e^{t\\cot(t)-1}\\) twice and again solve for the limit as \\(t\\to0\\), you will obtain a value of -1. Because the term has even symmetry and negative curvature, the term must be approaching 1 from the negative end. Thus, the limit that we were originally interested in is:\n$$\\lim_{t\\to1^{-}}\\ln\\left(1-t\\right)=-\\infty$$This checks out because \\(t\\) is not defined above 1 on the reals (we can not approach from the positive side). This shows that the integrand goes to \\(-\\infty\\) at this point, and special measures may be useful for taking this into account when numerically approximating the integral.\nIt turns out that a simple substitution works nicely to tame the singularity. If we define the integrand as a function \\(I\\):\n$$y=-\\frac{1}{\\ln\\left(x\\right)\\pi}\\int_{0}^{\\pi}I\\left(x,t\\right)dt,$$ $$I\\left(x,t\\right):=\\ln\\left(1-\\frac{1}{t}\\ln\\left(x\\right)\\sin\\left(t\\right)e^{t\\cot\\left(t\\right)}\\right)$$And make the substitution \\(t=u^{2},\\ dt=2u\\ du\\), then the integral becomes:\n$$y=-\\frac{1}{\\ln\\left(x\\right)\\pi}\\int_{0}^{\\sqrt{\\pi}}2uI\\left(x,u^{2}\\right)du$$ $$=-\\frac{2}{\\pi\\ln(x)}\\int_{0}^{\\sqrt{\\pi}}t\\ln\\left(1-\\frac{1}{t^{2}}\\sin\\left(t^{2}\\right)e^{t^{2}\\cot\\left(t^{2}\\right)}\\ln(x)\\right)dt$$This integrand looks like this:\nThe level curves at fixed \\(x\\) are very nice. Note now, however, that this integrand very rapidly diverges to \\(\\infty\\) as \\(x\\to0^{+}\\). This is not obvious from the graph because this occurs for extremely small \\(x\\), but you can prove it easily; the integrand is of the form \\(t\\ln\\left(1-f\\left(t\\right)\\ln\\left(x\\right)\\right)\\), and \\(\\lim_{x\\to0^{+}}\\ln\\left(x\\right)=-\\infty\\). Thus you would want to use this integrand only when \\(x\\) is close to \\(\\sqrt[e]{e}\\), and the other integrand otherwise.\nThis completes the method of solving the problem using numerical integration. In the remainder of this blog post, I will use the Newton-Raphson method (which I have discussed briefly in the previous blog post) to solve the problem.\nNewton-Raphson The general strategy that I employ for solving the problem is turning it into a root-finding problem, which we can use the Newton-Raphson method to solve provided we have a decent enough initial approximation of the curve. I will use the simpler curve \\( x^y=y \\) for the convergent region \\(x\\in\\left(e^{-e},\\sqrt[e]{e}\\right)\\) due to its minimal computational complexity. I will use the more complicated curve \\(x^{x^y}=y\\) for the branching region \\(x\\in\\left(0,e^{-e}\\right)\\) due to its more complete structure.\nWe can rearrange the equations to create root-finding problems:\n$$0=x^{y}-y=:g_{1}(y)$$ $$0=x^{y}\\ln\\left(x\\right)-\\ln\\left(y\\right)=g_{2}(y)$$I obtained the second equation by first taking the natural log of both sides to collapse the exponents. What we’ve constructed are functions \\(g_{1,2}(y)\\), provided some \\(x\\), which is 0 only when \\(y\\) is the infinite tetration of \\(x\\). For this problem, we can use the Newton-Raphson method, which I’ve briefly discussed in a previous blog post. In essence, to find \\(z\\) such that \\(g(z)=0\\), we compute the fixed-point iteration \\(z\\to f(z)\\) where\n$$z\\mapsto f\\left(z\\right)=z-\\frac{g\\left(z\\right)}{g'\\left(z\\right)}$$And you can easily find the derivative with the quotient rule, which determines convergence via the Banach fixed-point theorem:\n$$-1","wordCount":"4057","inLanguage":"en","datePublished":"2025-03-28T18:00:00-08:00","dateModified":"2025-03-28T18:00:00-08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.mileswaugh.com/posts/infinite_tetration/"},"publisher":{"@type":"Organization","name":"Waugh Blog","logo":{"@type":"ImageObject","url":"https://blog.mileswaugh.com/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://blog.mileswaugh.com/ accesskey=h title="Waugh Blog (Alt + H)">Waugh Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Efficiently Computing Infinite Tetration</h1><div class=post-meta><span title='2025-03-28 18:00:00 -0800 -0800'>March 28, 2025</span>&nbsp;·&nbsp;<span>20 min</span>&nbsp;·&nbsp;<span>4057 words</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#the-na%c3%afve-approach aria-label="The Naïve Approach">The Naïve Approach</a></li><li><a href=#structure-analysis aria-label="Structure Analysis">Structure Analysis</a><ul><li><a href=#fixed-point-iteration aria-label="Fixed-Point Iteration">Fixed-Point Iteration</a></li><li><a href=#plotting-behavior aria-label="Plotting Behavior">Plotting Behavior</a></li><li><a href=#deriving-key-points aria-label="Deriving Key Points">Deriving Key Points</a></li></ul></li><li><a href=#lambert-w aria-label="Lambert W">Lambert W</a></li><li><a href=#newton-raphson aria-label=Newton-Raphson>Newton-Raphson</a><ul><li><a href=#results aria-label=Results>Results</a></li></ul></li></ul></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>In this blog post, I derive two efficient methods to stably compute the infinite tetration (or &ldquo;power tower&rdquo;). &ldquo;Tetration&rdquo; simply means repeated exponentiation, so &ldquo;infinite tetration&rdquo; simply means exponentiation an infinite number of times. The infinite tetration of \(x\), which I will call \(y\), is defined as:</p>$$y=x^{x^{x^{.^{.^{.}}}}}$$<p>I will begin with a simple &ldquo;naïve&rdquo; approach to solving the problem. Then, I will more deeply analyze the structure of the problem. From there, I will discuss two approaches to solving the problem efficiently.</p><p>The first method I derive turns the problem into an integral which can be solved numerically. The second method that I derive, and the main focus of this blog post, is an iterative method that achieves stable quadratic convergence for all valid \(x\). &ldquo;Iterative&rdquo; simply means that the method constructs successively more accurate approximations until the approximation error is within a desired tolerance of the true solution.</p><hr><h2 id=the-naïve-approach>The Naïve Approach<a hidden class=anchor aria-hidden=true href=#the-naïve-approach>#</a></h2><p>The most straightforward way to compute the infinite power tower \(y\) of an input \(x\) is to repeatedly raise \(x\) to the power of more \(x\)&rsquo;s. The idea is that the <em>infinite</em> power tower, which we can not compute (it requires an infinite number of operations), is sufficiently similar to a <em>very large</em> power tower, which we can compute. As we will discover later, however, there are certain inputs where this approach is problematic. It is important to note that repeated exponentiation is <em>right-associative</em>, not <em>left-associative</em>. That means \(a^{b^{c}}=a^{\left(b^{c}\right)}\neq\left(a^{b}\right)^{c}\). Mathematically, the Naïve approach is then:</p>$$y_0 = x$$<p></p>$$y_1 = x^x = x^{y_0}$$<p></p>$$y_2 = x^{x^{x}} = x^{y_1}$$<p></p>$$\vdots$$<p></p>$$y_n = x^{y_{n-1}}$$<p>In the limit as we raise \(x\) to the power of itself more times, it converges on the solution \(y\) if one exists. However, this approach converges quite slowly for many values of \(x\). Furthermore, for small \(x\), the sequence oscillates indefinitely, and the values between which it oscillates converge.</p><p>To see how inefficient this is for some inputs, look at this example of convergence of 0.4 raised to the power of itself repeatedly:</p><table><thead><tr><th>n</th><th>\(y_n\)</th><th>Absolute error ( \(\left | y_n-y_{\infty}\right | \) )</th></tr></thead><tbody><tr><td>0</td><td>0.40000000000</td><td>0.18504317197</td></tr><tr><td>1</td><td>0.69314484316</td><td>0.10810167118</td></tr><tr><td>2</td><td>0.52987073647</td><td>0.05517243550</td></tr><tr><td>3</td><td>0.61537979747</td><td>0.03033662550</td></tr><tr><td>4</td><td>0.56900457469</td><td>0.01603859728</td></tr><tr><td>5</td><td>0.59370446436</td><td>0.00866129238</td></tr><tr><td>&mldr;</td><td>&mldr;</td><td>&mldr;</td></tr><tr><td>35</td><td>0.58504317204</td><td>0.00000000007</td></tr><tr><td>36</td><td>0.58504317194</td><td>0.00000000003</td></tr><tr><td>37</td><td>0.58504317199</td><td>0.00000000002</td></tr><tr><td>38</td><td>0.58504317196</td><td>0.00000000001</td></tr><tr><td>39</td><td>0.58504317198</td><td>0.00000000001</td></tr></tbody></table><p>Here is an example of &ldquo;oscillatory&rdquo; behavior with 0.02 raised to the power of itself repeatedly:</p><table><thead><tr><th>n</th><th>\(y_n\)</th></tr></thead><tbody><tr><td>0</td><td>0.0200000000</td></tr><tr><td>1</td><td>0.9247420362</td></tr><tr><td>2</td><td>0.0268467067</td></tr><tr><td>3</td><td>0.9003020739</td></tr><tr><td>&mldr;</td><td>&mldr;</td></tr><tr><td>26</td><td>0.0314614034</td></tr><tr><td>27</td><td>0.8841949279</td></tr><tr><td>28</td><td>0.0314614938</td></tr><tr><td>29</td><td>0.8841946152</td></tr><tr><td>30</td><td>0.0314615323</td></tr></tbody></table><p>As we approach an infinite number of iterations, the sequence tends toward bouncing between two values. Of course, you might imagine that large numbers raised to the power of themselves will keep getting larger forever. Here is an example of 1.5 raised to itself repeatedly, where the infinite tetration spirals to infinity:</p><table><thead><tr><th>n</th><th>\(y_n\)</th></tr></thead><tbody><tr><td>1</td><td>1.500000000</td></tr><tr><td>2</td><td>1.837117307</td></tr><tr><td>3</td><td>2.106203352</td></tr><tr><td>&mldr;</td><td>&mldr;</td></tr><tr><td>9</td><td>4.382546732</td></tr><tr><td>10</td><td>5.911914873</td></tr><tr><td>11</td><td>10.99098293</td></tr><tr><td>12</td><td>86.18189174</td></tr><tr><td>13</td><td>1,499,263,005,586,587</td></tr><tr><td>14</td><td>5.400339825 × 10^264,007,110,309,345</td></tr></tbody></table><p>By the 14th iteration, it is quite evident that the sequence tends to infinity.</p><p>To better understand these types of behaviors, I&rsquo;ve plotted the true infinite tetration curve below, in green, alongside the naïve approach for computing it, in red:</p><p><img alt="Animation of the naïve approach of computing the infinite tetration." loading=lazy src=/posts/infinite_tetration/media/gif/naive_1.gif></p><p>Notice how, for small values below a threshold between 0 and 0.2, the numbers oscillate between two values in the limit as the number of iterations approaches infinity. These two values are denoted by the dashed green line. In this region, the infinite tetration is technically not defined, because it does not converge to a single value. Above that &ldquo;branching point,&rdquo; until a point a bit greater than 1.4, the infinite tetration does converge to a single value. These are denoted by the solid green line. Finally, above some point between 1.4 and 1.5, the infinite tetration diverges toward infinity.</p><p>Given these observations, I would like to answer the following questions:</p><ul><li>When does the infinite tetration converge? When does it oscillate indefinitely? When does it blow up to infinity? Can we rigorously derive exactly where these transitions occur?</li><li>Can we find a &ldquo;better&rdquo; way to compute the result of the infinite tetration (either what value the sequence converges to, or between what values it oscillates between)?</li></ul><p>Moving forward in this blog post, I will neglect the infinite tetration of negative numbers. This is because, inevitably, you will encounter complex numbers; I wish to remain within the reals this time. The only exception to this is the infinite tetration of -1, which is -1. And that&rsquo;s rather boring.</p><p>In the next section, <strong>Structure Analysis</strong>, I will address the first bullet point. In the following two sections, <strong>Lambert W</strong> and <strong>Newton-Raphson</strong>, I will address the second bullet point.</p><hr><h2 id=structure-analysis>Structure Analysis<a hidden class=anchor aria-hidden=true href=#structure-analysis>#</a></h2><h3 id=fixed-point-iteration>Fixed-Point Iteration<a hidden class=anchor aria-hidden=true href=#fixed-point-iteration>#</a></h3><p>Recall the naïve approach:</p>$$y_n = x^{y_{n-1}}$$<p>We say that the sequence &ldquo;converges&rdquo; if the limit \(y_{\infty}=\lim_{n\to\infty}y_n\) exists. If the sequence does indeed converge, then the solution we seek, \(y_{\infty}\), is unchanged when \(x\) is raised to its power (in both cases, \(x\) is raised to itself an infinite number of times):</p>$$y_{\infty}=x^{y_{\infty}}$$<p>Alternatively, we can think of the naïve approach as creating a variable \(z\) which is initialized to \(x\), and is updated by applying the function:</p>$$z\to f\left(z\right)$$<p>Where \(f\left(z\right)=x^z\). If we repeatedly apply this, then we hope that \(z\) will approach \(y_\infty\) if \(y_{\infty}\) does exist. Our problem, then, is to find a &ldquo;fixed point&rdquo; of \(f\): that is, a value \(z\) such that \(f\left(z\right)=z\). This is called a fixed point because the value of \(z\) remains fixed under application of the function \(f\). Repeatedly passing the variable through the function and setting the variable to the resulting value is called a fixed-point iteration. Often, we can find fixed points of a function by simply repeatedly applying the fixed-point iteration. The variable will keep changing until it reaches the fixed point, after which it will remain at the fixed point.</p><p>For instance, take the function \(f\left(z\right)=\sin\left(z\right)+z\). It is not difficult to realize that this function has an infinite number of fixed points at \(z=n\pi\) for all \(n\in\mathbb{Z}\). At these points, \(\sin\left(z\right)\) is 0, so the function simply returns \(z\). If we approximate the fixed point as \(3\), we can write some simple Python code and watch as the fixed-point iteration converges quickly on \(\pi\):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>import</span> math
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> <span style=color:#ae81ff>3.0</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;z_0: </span><span style=color:#e6db74>{</span>z<span style=color:#e6db74>:</span><span style=color:#e6db74>.11f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>4</span>):
</span></span><span style=display:flex><span>    z <span style=color:#f92672>+=</span> math<span style=color:#f92672>.</span>sin(z)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;z_</span><span style=color:#e6db74>{</span>i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74>: </span><span style=color:#e6db74>{</span>z<span style=color:#e6db74>:</span><span style=color:#e6db74>.11f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;π:   </span><span style=color:#e6db74>{</span>math<span style=color:#f92672>.</span>pi<span style=color:#e6db74>:</span><span style=color:#e6db74>.11f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><p>Which gives the following output:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-zsh data-lang=zsh><span style=display:flex><span>z_0: 3.00000000000
</span></span><span style=display:flex><span>z_1: 3.14112000806
</span></span><span style=display:flex><span>z_2: 3.14159265357
</span></span><span style=display:flex><span>z_3: 3.14159265359
</span></span><span style=display:flex><span>z_4: 3.14159265359
</span></span><span style=display:flex><span>π:   3.14159265359
</span></span></code></pre></div><p>Wow, it converged with just 3 iterations! In fact, this method converges with <em>cubic convergence</em>. I will not explain in detail what that means and why that is the case as it is out of the scope of this post, but the reason ends up being because \(0=f&rsquo;\left(\pi\right)=f&rsquo;&rsquo;\left(\pi\right)\ne f^{\left(3\right)}\left(\pi\right)=1\). This method for computing pi is generally not very useful in practice, however, because the computer has to compute a Taylor series for the sine function, which involves a lot of floating-point operations per function call. We may as well just compute \(\pi=4\arctan\left(1\right)\) if we&rsquo;re using trig functions at all, which is just a single function call.</p><p>Hopefully I&rsquo;ve convinced you that fixed-point iteration can be very useful at times. However, we have no reason to believe that fixed-point iteration will always lead us to the fixed point of the function. Consider the function \(f\left(z\right)=z^2-2\) as an example. The fixed point \(p\) of the function \(f\) is easy to solve for:</p>$$p=p^2-2$$<p></p>$$p^{2}-p-2=0$$<p></p>$$\left(p-2\right)\left(p+1\right)=0$$<p></p>$$p=2,-1$$<p>Let&rsquo;s say we estimate \(p\) to be about \(z_0=1.5\). It&rsquo;s between the two fixed points, so surely it will converge on one. Right? We can write some simple Python code to perform the fixed-point iteration:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>z <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.5</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>15</span>):
</span></span><span style=display:flex><span>    z <span style=color:#f92672>=</span> z <span style=color:#f92672>*</span> z <span style=color:#f92672>-</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>z<span style=color:#e6db74>:</span><span style=color:#e6db74>+0.3f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><p>This yields the following output:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-diff data-lang=diff><span style=display:flex><span><span style=color:#a6e22e>+0.250
</span></span></span><span style=display:flex><span><span style=color:#a6e22e></span><span style=color:#f92672>-1.938
</span></span></span><span style=display:flex><span><span style=color:#f92672></span><span style=color:#a6e22e>+1.754
</span></span></span><span style=display:flex><span><span style=color:#a6e22e>+1.076
</span></span></span><span style=display:flex><span><span style=color:#a6e22e></span><span style=color:#f92672>-0.842
</span></span></span><span style=display:flex><span><span style=color:#f92672>-1.291
</span></span></span><span style=display:flex><span><span style=color:#f92672>-0.332
</span></span></span><span style=display:flex><span><span style=color:#f92672>-1.889
</span></span></span><span style=display:flex><span><span style=color:#f92672></span><span style=color:#a6e22e>+1.570
</span></span></span><span style=display:flex><span><span style=color:#a6e22e>+0.465
</span></span></span><span style=display:flex><span><span style=color:#a6e22e></span><span style=color:#f92672>-1.783
</span></span></span><span style=display:flex><span><span style=color:#f92672></span><span style=color:#a6e22e>+1.181
</span></span></span><span style=display:flex><span><span style=color:#a6e22e></span><span style=color:#f92672>-0.606
</span></span></span><span style=display:flex><span><span style=color:#f92672>-1.633
</span></span></span><span style=display:flex><span><span style=color:#f92672></span><span style=color:#a6e22e>+0.668
</span></span></span></code></pre></div><p>It went all over the place! Using the fixed point iteration did not converge to any fixed points of the function. Maybe it would help if we provided an initial estimate that was very close to the value. Let us now begin with an initial guess very close to 2:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>z <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.999</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>15</span>):
</span></span><span style=display:flex><span>    z <span style=color:#f92672>=</span> z <span style=color:#f92672>*</span> z <span style=color:#f92672>-</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>z<span style=color:#e6db74>:</span><span style=color:#e6db74>+0.3f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><p>But this does not converge to 2 either:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-diff data-lang=diff><span style=display:flex><span><span style=color:#a6e22e>+1.996
</span></span></span><span style=display:flex><span><span style=color:#a6e22e>+1.984
</span></span></span><span style=display:flex><span><span style=color:#a6e22e>+1.936
</span></span></span><span style=display:flex><span><span style=color:#a6e22e>+1.749
</span></span></span><span style=display:flex><span><span style=color:#a6e22e>+1.060
</span></span></span><span style=display:flex><span><span style=color:#a6e22e></span><span style=color:#f92672>-0.876
</span></span></span><span style=display:flex><span><span style=color:#f92672>-1.233
</span></span></span><span style=display:flex><span><span style=color:#f92672>-0.479
</span></span></span><span style=display:flex><span><span style=color:#f92672>-1.771
</span></span></span><span style=display:flex><span><span style=color:#f92672></span><span style=color:#a6e22e>+1.135
</span></span></span><span style=display:flex><span><span style=color:#a6e22e></span><span style=color:#f92672>-0.711
</span></span></span><span style=display:flex><span><span style=color:#f92672>-1.494
</span></span></span><span style=display:flex><span><span style=color:#f92672></span><span style=color:#a6e22e>+0.233
</span></span></span><span style=display:flex><span><span style=color:#a6e22e></span><span style=color:#f92672>-1.946
</span></span></span><span style=display:flex><span><span style=color:#f92672></span><span style=color:#a6e22e>+1.786
</span></span></span></code></pre></div><p>In fact, unless your initial estimate is exactly the fixed point, the fixed-point iteration will never converge on either of the fixed points! From this, it follows that we may want to distinguish between two types of fixed points: the type that we will converge on via the fixed-point iteration, and the type that we will never converge on.</p><hr><p>It is finally time to characterize fixed points. If a fixed point has a <a href=https://en.wikipedia.org/wiki/Neighbourhood_(mathematics)>neighborhood</a> of points around it that converge on it, we call that fixed-point <strong>attracting</strong>. The largest such neighborhood of that point is called the <strong>basin of attraction</strong>. You might guess that the fixed point 2 in the second example is not attracting, whereas the fixed point \(\pi\) from the first example is attracting.</p><p>Similarly, there is a notion of <em>repelling</em> fixed points, such as the fixed points in the second example. Even if your guess is very close to 2, the iteration will repel the guess away from 2. Finally, there is a notion of a <em>periodic</em> point, which is some value that the system will periodically return to. The points that the repeated tetration oscillates between in the limit as the iteration count approaches infinity are examples of periodic points. In this post, we will primarily be concerned with whether a fixed point is attracting or not, since that determines convergence. We will also be interested in finding an attracting fixed point&rsquo;s basin of convergence, because that is a region within which an initial guess is theoretically guaranteed to converge.</p><p>This begs the question: is there a way to definitively <em>prove</em> whether a fixed point is attracting? Furthermore, can we determine its basin of attraction? This is exactly what the <strong>Banach–Caccioppoli fixed-point theorem</strong> (1922) allows us to do, provided that the fixed-point iteration \(f\) is continuous. Often, &ldquo;Caccioppoli&rdquo; is omitted when referencing the name of the theorem. While not strictly necessary, it also helps mathematically if the fixed-point iteration is differentiable. You&rsquo;ll see why in a moment.</p><p>I will briefly note, however, that the theorem also allows us to prove whether a fixed point exists in a region of the function&rsquo;s domain, and whether that fixed point is the only (&ldquo;unique&rdquo;) fixed point in that region. This can be useful, for instance, if we can&rsquo;t solve for the fixed points of a function. I will skip this part of the theorem entirely as it is not relevant to this blog post, but I may revisit it in a future post.</p><p>Generally, the function \(f\) is defined on a &ldquo;metric space&rdquo; \(\left(X,d\right)\). You can think of this as a set of points \(X\) equipped with the additional structure of a &ldquo;distance,&rdquo; \(d\), defined between points. \(d\) itself is a function that takes in two points from our space and outputs a nonnegative number, called the &ldquo;distance&rdquo; between the points. This distance function can be anything as long as it satisfies four simple axioms. I will briefly state them here in plain English, but they are not relevant to this post:</p><ol><li>The distance from a point to itself is zero.</li><li>The distance between two different points is positive.</li><li>The distance from x to y is the distance from y to x.</li><li>The distance between x and z can never be greater than the sum of distances between x and y and between y and z.</li></ol><p>The fixed-point iteration \(f\) must necessarily map \(f: X\to X\) (it maps from a space onto the same space). This requirement should make intuitive sense: for a fixed point to be a fixed point, the function should not affect it. The fixed point \(p\) satisfies \(f(p)=p\), so \(f(p)\) and \(p\) are both in \(X\).</p><p>What the Banach fixed-point theorem states is that a fixed point \(p\) of \(f\) will be attracting if \(f\) is a <strong>contraction mapping</strong> in some neighborhood around \(p\). Then, the largest such neighborhood is the basin of attraction. We say that \(f\) is a contraction mapping if there exists a constant \(0\leq k&lt;1\) such that for all \(x,y\in X\), the following inequality holds:</p>$$d\left(f(x),f(y)\right)\leq k\cdot d(x,y)$$<p>Intuitively, we choose two points in the space and apply the function to both. If the distances between the points contract under the fixed-point iteration for all pairs of points that we choose, then the function is a contraction mapping. It should make sense why this theorem is true: if the function application does not contract points into the fixed point, then points will never converge to the fixed point. But if the function application does contract points into the fixed point, then points will converge to the fixed point. I leave the proof of the theorem as an exercise to the reader, as it is a relatively straightforward proof.</p><p>In our case, because \(f\) is a map \(f:\mathbb{R}\to\mathbb{R}\), we can use the Euclidean distance as our metric:</p>$$d(x,y)=\left|x-y\right|$$<p>The Banach fixed-point theorem states that our fixed-point iteration will be contractive over some domain \(X\subseteq\mathbb{R}\) if there exists a constant \(0\leq k&lt;1\) such that, for all \(x,y\in X\), the following inequality holds:</p>$$\left|f(x)-f(y)\right|\leq k\cdot\left|x-y\right|$$<p>Using the mean value theorem, we can rewrite this inequality as:</p>$$\left|f'(z)\right|\leq k$$<p>Where \(z\) is some point between \(x\) and \(y\). This is a much more convenient form of the inequality to check. You can intuitively think of this as checking that the condition described is satisfied for the subset of all pairs of points that are infinitesimally close. If you wish to check the distance between two points that are a finite distance apart, then you must sum the distances between all the infinitesimally close points that lie between the two points. If all of the infinitesimal distances contract, then the finite distance which is their sum will also contract. If we can find a constant \(0\leq k&lt;1\) such that the derivative of the fixed-point iteration is less than \(k\) for all points in a neighborhood of a fixed point, then we can conclude that the fixed-point attracting.</p><p>With my example of \(f(z)=z^2-2\), the fixed point 2 is not attracting because, where \(f&rsquo;(z)=2z\):</p>$$\left|f'(2)\right| = 4 > 1 > k$$<p>The derivative is only less than 1 when \(z\in(-\frac{1}{2},\frac{1}{2})\), which does not contain any fixed points of the function. Therefore, all fixed points of the function are not attracting, and the fixed-point iteration will not work.</p><p>This theorem will serve us well soon enough, since we observed that the infinite tetration can be represented as a fixed point of a function.</p><hr><h3 id=plotting-behavior>Plotting Behavior<a hidden class=anchor aria-hidden=true href=#plotting-behavior>#</a></h3><p>Back to the infinite tetration: let&rsquo;s make some plots. The first step is to express the infinite tetration \(y\) of \(x\) recursively as:</p>$$x^y=y$$<p>If we plot this equation, we get the following red curve. Ignore the green curve for now (we&rsquo;ll talk about it shortly):</p><p><img alt="Graph of x^y=y" loading=lazy src=/posts/infinite_tetration/media/svg/branches.svg></p><p>Interestingly, if we instead graph the application of two iterations:</p>$$x^{x^y}=y$$<p>Then we get the same curve, but with an additional two <a href=https://mathworld.wolfram.com/Branch.html>branches</a> when x is small, colored above as the green curve. These two green branches correspond to the values which the naïve sequence oscillates between in the limit as the number of iterations approaches infinity. When \(x\) is larger than where the green curve exists, the sequence will converge on the red curve. Finally, to the right of the point where the red curve bends backward (to the left, where there now appear to be multiple values for the tetration for a single input), which is also the upper bound on the domain of the curve, all values will diverge to infinity.</p><p>Why do the branches only appear with the more complicated expression? If you compute the iteration only once, then in the oscillatory region, the value will flip to the other value between which the sequence is oscillating. But if you apply the iteration twice, then the value will flip twice, back to its original value. The first graph plots what is invariant under a single iteration, while the second graph plots what is invariant under two iterations.</p><p>I want to briefly also talk about the red-colored &ldquo;middle&rdquo; branch that lies between the two green branches. We observed before that repeatedly exponentiating \(x\) to its own power in this oscillatory region causes the value to oscillate between the two green branches. The middle branch is sort of a &ldquo;ghost&rdquo; solution—it satisfies the property that raising \(x\) to its values returns the same values (which is consistent with how we described the infinite tetration), but if we begin with \(x\) and repeatedly construct a larger and larger power tower, we will never reach this branch. Thus, I am not sure whether to call this a &ldquo;valid solution&rdquo; to the infinite tetration or not.</p><hr><h3 id=deriving-key-points>Deriving Key Points<a hidden class=anchor aria-hidden=true href=#deriving-key-points>#</a></h3><p>Let&rsquo;s establish some important points on the curve. The trivial cases are when \(x=0\) and \(x=1\). When \(x=0\), the infinite tetration oscillates between 0 and 1. This is because anything raised to the power of 0 is 1, and anything raised to the power of 1 is unaffected. So \(0^0=1\), \(0^{0^0}=0^{1}=0\), and the cycle repeats. Raising 1 to the power of itself is just 1, so the infinite tetration of 1 is 1.</p><p>Let&rsquo;s find out where the curve goes from converging to diverging. The curve is vertical when \(\frac{dx}{dy}=0\). We can solve for \(x\):</p>$$x^y=y\implies x = \sqrt[y]{y}$$<p>And differentiate:</p>$$\frac{dx}{dy}=-y^{\frac{1}{y}-2}(\ln(y)-1)=0$$<p>Which is true when \(y^{\frac{1}{y}-2}=0\) and when \(\ln(y)=1\). Thus, the curve is vertical when \(y=0\) and when \(y=e\). That means the curve is vertical when \(x=0\) and \(x=\sqrt[e]{e}\). This tells us when the infinite tetration will diverge to infinity: whenever we are infinitely tetrating a number greater than \(\sqrt[e]{e}\). The largest value that can be infinitely-tetrated is \(\sqrt[e]{e}\), converging to the largest infinitely-tetrated value, \(e\).</p><p>To find when the curve transitions from oscillating to converging to a single value, we can use the Banach fixed-point theorem. The derivative of the fixed-point iteration \(f\left(y\right)=x^{y}\) is:</p>$$f'\left(y\right)=\frac{d}{dy}\left[x^{y}\right]=x^{y}\ln\left(x\right)$$<p>Because the basin of attraction is where the magnitude of the derivative is less than 1, the boundary of the basin is when the derivative is exactly 1 or -1. If we consider \(0 &lt; x &lt; 1\), then \(\ln(x)\) is strictly negative and \(x^y\) is strictly positive, meaning g&rsquo;(y) is strictly negative. That means we can simply check when \(g&rsquo;(y) = -1\). Thus, we are interested in the values of \(x\) such that:</p>$$x^{y}\ln\left(x\right)=-1\implies y=\frac{\ln\left(-\frac{1}{\ln\left(x\right)}\right)}{\ln\left(x\right)}$$<p>We also know, additionally, that \(x^{y}=y\), so:</p>$$x^{\frac{\ln\left(-\frac{1}{\ln\left(x\right)}\right)}{\ln\left(x\right)}}=\frac{\ln\left(-\frac{1}{\ln\left(x\right)}\right)}{\ln\left(x\right)}$$<p></p>$$\implies-\frac{1}{\ln(x)}=\frac{\ln\left(-\frac{1}{\ln(x)}\right)}{\ln(x)}$$<p></p>$$\implies \frac{\ln\left(-\frac{1}{\ln(x)}\right)+1}{\ln(x)}=0$$<p></p>$$\implies x=e^{-e}$$<p>And</p>$$y=\frac{\ln\left(-\frac{1}{\ln\left(e^{-e}\right)}\right)}{\ln\left(e^{-e}\right)}=\frac{1}{e}$$<p>Indeed, the curve is undefined on \(\left(-\infty,0\right)\), oscillates on \(\left[0,e^{-e}\right)\), converges on \(\left[e^{-e},\sqrt[e]{e}\right]\), and diverges on \(\left(\sqrt[e]{e},\infty\right)\).</p><p>What we would like to do is formulate a method that efficiently converges to all branches in order to determine what the infinite tetration converges to, what it tends toward, or what it oscillates between.</p><hr><p>Before we move onto the next section, I want to briefly provide very precise upper and lower bounds for what an input \(x\) could be given some infinite tetration \(y\) in the convergent region. I&rsquo;ve obtained these via <a href=https://mathworld.wolfram.com/PadeApproximant.html>Padé approximation</a>:</p>$$\frac{5+\left(77y-40\right)y}{13+\left(\left(4y+35\right)y-10\right)y}\le x\le3\cdot\frac{399+\left(\left(279y+3721\right)y-2379\right)y}{1072-\left(133+\left(\left(89y-1709\right)y-3501\right)y\right)y}$$<p>If you are on a small device such as a phone, you can scroll to see the entire formula. These bounds are very accurate:</p><p><img alt="Graph of the rational bounds along with the infinite tetration curve." loading=lazy src=/posts/infinite_tetration/media/svg/bounds.svg></p><p>The infinite tetration curve is plotted in blue, the bounds are in orange, and the box of convergence is in green. The bounds are too accurate in the convergence box, so it is difficult to plot it. Here is only the convergence region:</p><p><img alt="Only the convergence region" loading=lazy src=/posts/infinite_tetration/media/svg/bounds_2.svg></p><p>While I will not use them in this blog post, feel free to use them if you work on this subject any more.</p><hr><h2 id=lambert-w>Lambert W<a hidden class=anchor aria-hidden=true href=#lambert-w>#</a></h2><p>Our first step should be to attempt to solve for the infinite tetration, \( y \), directly. However, in order to solve for \(y\) exactly, we require the <strong>Lambert W function</strong> \(W(z)\), which can not be expressed using only elementary functions. The Lambert W function is the function that satisfies:</p>$$W(z) e^{W(z)} = z$$<p>You can get this result by rearranging \(y=x^y\) into</p>$$\ln\left(\frac{1}{y}\right)e^{\ln\left(\frac{1}{y}\right)}=-\ln\left(x\right)$$<p>And, by the definition of the Lambert W function:</p>$$W\left(-\ln\left(x\right)\right)e^{W\left(-\ln\left(x\right)\right)}=-\ln\left(x\right)$$<p></p>$$\implies W\left(-\ln\left(x\right)\right)=\ln\left(\frac{1}{y}\right)$$<p>From here, we can obtain the solution in terms of the Lambert W function:</p>$$y=-\frac{W\left(-\ln\left(x\right)\right)}{\ln\left(x\right)}$$<p>Because we are left with the Lambert W function, which is irreducible, this motivates a numerical approach. The presence of that function demonstrates that it is impossible to exactly represent a formula in closed form. If our problem could be solved exactly using a finite number of elementary operations and functions, then so would the Lambert W function, which we know is not the case.</p><p>One potential technique to solve the problem is to compute the Lambert W function. We can employ an integral to compute the Lambert W function that was derived by <a href=https://www.uwo.ca/apmaths/faculty/jeffrey/pdfs/BersteinPick.pdf>Kalugin, Jeffrey, and Corless</a> in 2011, giving:</p>$$y=-\frac{1}{\ln\left(x\right)\pi}\int_{0}^{\pi}\ln\left(1-\frac{1}{t}\ln\left(x\right)\sin\left(t\right)e^{t\cot\left(t\right)}\right)dt$$<p>Note that this integral gives only the center branch present in \(y=x^y\). The integrand is reasonably well-behaved and looks like this:</p><p><img alt="Integrand of the Lambert W function integral derived in 2011" loading=lazy src=/posts/infinite_tetration/media/raster/W_integrand.jpg></p><p>At the corner where \(x=\sqrt[e]{e}\) and \(t=0\), the integrand diverges to \(-\infty\). You can check this by substituting in the value for \(x\) and taking \(t\to0^{+}\) in the limit to get:</p>$$\lim_{t\to0^{+}}\ln\left(1-\frac{1}{t}\sin(t)e^{t\cot(t)-1}\right)$$<p>We&rsquo;re interested in the limit of the t-dependent term:</p>$$\lim_{t\to0^{+}}\frac{1}{t}\sin(t)e^{t\cot(t)-1}$$<p>Notice that \(\sin\left(x\right)\), \(x\), and \(\cot\left(x\right)\) are all odd functions, so \({\sin\left(x\right)}/{x}\) and \(x\cot\left(x\right)\) are even, making the whole expression even. Thus, instead of only considering the right-hand limit, I will consider the limit from both sides (for simplicity, because the RH and LH limits equal each other).</p><p>Using L&rsquo;Hôpital&rsquo;s rule:</p>$$\lim_{t\to0}\frac{\sin(2t)-t}{\sin\left(t\right)}e^{t\cot(t)-1}$$<p></p>$$=\lim_{t\to0}\frac{\sin(2t)-t}{\sin\left(t\right)}\cdot\lim_{t\to0}e^{t\cot(t)-1}$$<p>Using L&rsquo;Hôpital&rsquo;s rule again, the left-hand limit becomes:</p>$$\lim_{t\to0}\left[4 \cos (t)-3 \sec (t)\right]=1$$<p>Using L&rsquo;Hôpital&rsquo;s rule again on \(t\cot(t)={t}/{\tan\left(t\right)}\), the right-hand limit becomes:</p>$$\lim_{t\to0}e^{\cos^{2}\left(t\right)-1}=1$$<p>If you differentiate the original term \(\frac{1}{t}\sin(t)e^{t\cot(t)-1}\) twice and again solve for the limit as \(t\to0\), you will obtain a value of -1. Because the term has even symmetry and negative curvature, the term must be approaching 1 from the negative end. Thus, the limit that we were originally interested in is:</p>$$\lim_{t\to1^{-}}\ln\left(1-t\right)=-\infty$$<p>This checks out because \(t\) is not defined above 1 on the reals (we can not approach from the positive side). This shows that the integrand goes to \(-\infty\) at this point, and special measures may be useful for taking this into account when numerically approximating the integral.</p><p>It turns out that a simple substitution works nicely to tame the singularity. If we define the integrand as a function \(I\):</p>$$y=-\frac{1}{\ln\left(x\right)\pi}\int_{0}^{\pi}I\left(x,t\right)dt,$$<p></p>$$I\left(x,t\right):=\ln\left(1-\frac{1}{t}\ln\left(x\right)\sin\left(t\right)e^{t\cot\left(t\right)}\right)$$<p>And make the substitution \(t=u^{2},\ dt=2u\ du\), then the integral becomes:</p>$$y=-\frac{1}{\ln\left(x\right)\pi}\int_{0}^{\sqrt{\pi}}2uI\left(x,u^{2}\right)du$$<p></p>$$=-\frac{2}{\pi\ln(x)}\int_{0}^{\sqrt{\pi}}t\ln\left(1-\frac{1}{t^{2}}\sin\left(t^{2}\right)e^{t^{2}\cot\left(t^{2}\right)}\ln(x)\right)dt$$<p>This integrand looks like this:</p><p><img alt="Transformed integrand." loading=lazy src=/posts/infinite_tetration/media/raster/transformed_integral.jpg></p><p>The level curves at fixed \(x\) are very nice. Note now, however, that this integrand very rapidly diverges to \(\infty\) as \(x\to0^{+}\). This is not obvious from the graph because this occurs for extremely small \(x\), but you can prove it easily; the integrand is of the form \(t\ln\left(1-f\left(t\right)\ln\left(x\right)\right)\), and \(\lim_{x\to0^{+}}\ln\left(x\right)=-\infty\). Thus you would want to use this integrand only when \(x\) is close to \(\sqrt[e]{e}\), and the other integrand otherwise.</p><p>This completes the method of solving the problem using numerical integration. In the remainder of this blog post, I will use the Newton-Raphson method (which I have discussed briefly in the previous blog post) to solve the problem.</p><hr><h2 id=newton-raphson>Newton-Raphson<a hidden class=anchor aria-hidden=true href=#newton-raphson>#</a></h2><p>The general strategy that I employ for solving the problem is turning it into a root-finding problem, which we can use the Newton-Raphson method to solve provided we have a decent enough initial approximation of the curve. I will use the simpler curve \( x^y=y \) for the convergent region \(x\in\left(e^{-e},\sqrt[e]{e}\right)\) due to its minimal computational complexity. I will use the more complicated curve \(x^{x^y}=y\) for the branching region \(x\in\left(0,e^{-e}\right)\) due to its more complete structure.</p><p>We can rearrange the equations to create root-finding problems:</p>$$0=x^{y}-y=:g_{1}(y)$$<p></p>$$0=x^{y}\ln\left(x\right)-\ln\left(y\right)=g_{2}(y)$$<p>I obtained the second equation by first taking the natural log of both sides to collapse the exponents. What we&rsquo;ve constructed are functions \(g_{1,2}(y)\), provided some \(x\), which is 0 only when \(y\) is the infinite tetration of \(x\). For this problem, we can use the Newton-Raphson method, which I&rsquo;ve briefly discussed in a previous blog post. In essence, to find \(z\) such that \(g(z)=0\), we compute the fixed-point iteration \(z\to f(z)\) where</p>$$z\mapsto f\left(z\right)=z-\frac{g\left(z\right)}{g'\left(z\right)}$$<p>And you can easily find the derivative with the quotient rule, which determines convergence via the Banach fixed-point theorem:</p>$$-1<f'\left(z\right)=\frac{\left|g\left(z\right)g''\left(z\right)\right|}{g'\left(z\right)^{2}}<1$$<p>Using the functions which we wish to find the roots of, we can construct the Newton-Raphson iterations:</p>$$z\mapsto f_{1}(z)=z\cdot\frac{\ln\left(z\right)-1}{Lz-1}$$<p>and</p>$$z\mapsto f_{2}(z)=z\cdot\left[1+\frac{\ln\left(z\right)-L\cdot x^{z}}{L^{2}\cdot z\cdot x^{z}-1}\right]$$<p>Where \(L=\ln(x)\) can be precomputed. As Python code, the update procedures are:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#75715e># Initialize before iteration</span>
</span></span><span style=display:flex><span>L <span style=color:#f92672>=</span> log(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Iteration for y = x^y:</span>
</span></span><span style=display:flex><span>z <span style=color:#f92672>*=</span> (log(z) <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>/</span> (L <span style=color:#f92672>*</span> z <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Iteration for y = x^x^y:</span>
</span></span><span style=display:flex><span>Lxz <span style=color:#f92672>=</span> L <span style=color:#f92672>*</span> x<span style=color:#f92672>**</span>z
</span></span><span style=display:flex><span>z <span style=color:#f92672>*=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> (log(z) <span style=color:#f92672>-</span> Lxz) <span style=color:#f92672>/</span> (z <span style=color:#f92672>*</span> L <span style=color:#f92672>*</span> Lxz <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><p>To compute the infinite tetration <code>z</code> of <code>x</code>.</p><p>Note that both methods will work in the main region where the infinite tetration converges, but only the latter method will work in the oscillatory region. In the oscillatory region, the method will tend to converge to whichever three branches the seed is closest to (but this is not always the case&ndash;check out my previous blog post on <a href=https://blog.mileswaugh.com/posts/complex_fractals_desmos/>Newton Fractals</a>). The derivative of the more complicated method is:</p>$$f_{2}'\left(z\right)=\frac{\left(z^{2}x^{z}L^{3}+1\right)\left(x^{z}L-\ln\left(z\right)\right)}{\left(zx^{z}L^{2}-1\right)^{2}}$$<p>This is what the convergence basin looks like, plotted in orange, behind the fixed-point infinite tetration curve, plotted in blue:</p><p><img alt="Graph of the basin of convergence of the iteration" loading=lazy src=/posts/infinite_tetration/media/svg/convergence_region.svg></p><p>In order to compute the tetration of a number \(x\), we must choose an initial guess \(y_0\) such that \(\left(x,y_0\right)\) is in the orange region and every point \(\left(x,y\right)\) for \(y\) between the initial guess \(y_0\) and the blue fixed-point \(y_{\infty}\) is also in the orange region.</p><p>Because we used the more &ldquo;complete&rdquo; curve \(x^{x^y}=y\), the fixed-point iteration \(f_{2}(y)\) can converge to any of the three branches present on the domain \(x\in\left(0,e^{-e}\right)\).</p><p>In order to use the method, however, we must provide the method with an initial guess that is close to the desired curve. We can do this by approximating the complex curve with a simpler curve.</p><p>For the main section on \(x\in\left(e^{-e},\sqrt[e]{e}\right)\), I employ a rational function approximation. If my approximation is \(R_{\beta}(x)\) where \(\beta\) are the parameters that determine a particular rational function, then my objective is to find those coefficients \(\beta\) that minimize the \(L_2\) norm of the error on the interval:</p>$$\min_{\beta}\int_{e^{-e}}^{\sqrt[e]{e}}\left[R_{\beta}\left(x\right)-y\left(x\right)\right]^{2}dx$$<p>Where \(y\) is the infinite tetration of \(x\). This approximation comes out to:</p>$$y\approx \frac{\left(1+2.051x\right)x-6.037}{\left(16.183-4.006x\right)x-15.134}$$<p>In retrospect, a minimax approximation rule would be more sensible in this context, but this approximation works great anyway. This is the approximation plotted below in red, with the actual infinite tetration curve plotted in blue, and the basin of convergence lightly shaded in orange:</p><p><img alt="Approximation of the main region of the infinite tetration curve." loading=lazy src=/posts/infinite_tetration/media/svg/main_seed.svg></p><p>I also wanted to approximate each of the branches in the oscillatory region. Sometimes, a mathematical regime is not needed to determine some &ldquo;optimal&rdquo; seed. Often, some educated guesswork suffices. In my case, I came up with these approximations based on the general shape of the curves that I was looking for. From top to bottom, the approximations are:</p>$$y_{1}\approx \frac{1}{e}\left(1+\left(e-1\right)e^{{e}/{2}}\sqrt{e^{-e}-x}\right)$$<p></p>$$y_{2}\approx e^{0.15e-1}x^{0.15}$$<p></p>$$y_{3}\approx \frac{1}{e}\left(1-\sqrt{1-e^{2e}x^{2}}\right)$$<p>These approximations look like this, plotted in red, with the actual infinite tetration curve plotted in blue, and the basin of convergence lightly shaded in orange:</p><p><img alt="Approximation of the three branches in the oscillatory region of the infinite tetration curve." loading=lazy src=/posts/infinite_tetration/media/svg/branch_seed.svg></p><hr><h3 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h3><p>In this last part, I will present the results of the approach. Recall the naïve approach:</p><p><img alt="Full naïve approach" loading=lazy src=/posts/infinite_tetration/media/gif/naive_1.gif></p><p>The derived Newton-Raphson iteration with my initial approximations converge much more quickly in difficult regions:</p><p><img alt="Full Newton Raphson approach" loading=lazy src=/posts/infinite_tetration/media/gif/newton_1.gif></p><p>In particular, compare the convergence of the naïve approach near the branching point:</p><p><img alt="Naïve approach near the branching point" loading=lazy src=/posts/infinite_tetration/media/gif/naive_2.gif></p><p>To the Newton-Raphson iteration:</p><p><img alt="Newton Raphson iteration near the branching point" loading=lazy src=/posts/infinite_tetration/media/gif/newton_2.gif></p><p>And compare the convergence of the naïve approach near the upper limit:</p><p><img alt="Naïve approach for large inputs" loading=lazy src=/posts/infinite_tetration/media/gif/naive_3.gif></p><p>To the Newton-Raphson iteration:</p><p><img alt="Newton Raphson for large inputs" loading=lazy src=/posts/infinite_tetration/media/gif/newton_3.gif></p><p>Here are the approaches overlaid for the main convergent section, with the Naïve approach in blue, the Newton-Raphson iteration in red, and the ground truth curve in green:</p><p><img alt="Both methods overlaid for the main convergent section" loading=lazy src=/posts/infinite_tetration/media/gif/overlaid.gif></p><p>Here is my full implementation of the method as a Python module:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>import</span> math
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>BRANCH_POINT: float <span style=color:#f92672>=</span> math<span style=color:#f92672>.</span>exp(<span style=color:#f92672>-</span>math<span style=color:#f92672>.</span>e)
</span></span><span style=display:flex><span>MAX_VALUE: float <span style=color:#f92672>=</span> math<span style=color:#f92672>.</span>exp(<span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> math<span style=color:#f92672>.</span>e)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_infinite_tetration</span>(
</span></span><span style=display:flex><span>    x: float,
</span></span><span style=display:flex><span>    MAX_ITER: int <span style=color:#f92672>=</span> <span style=color:#ae81ff>128</span>,
</span></span><span style=display:flex><span>    tolerance: float <span style=color:#f92672>=</span> <span style=color:#ae81ff>1e-9</span>,
</span></span><span style=display:flex><span>    verbose: bool <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>) <span style=color:#f92672>-&gt;</span> tuple[float, <span style=color:#f92672>...</span>]:
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Computes the infinite tetration of non-negative real numbers using the Newton-Raphson method.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        x (float): non-negative float to infinitely tetrate.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        MAX_ITER (int, optional): Number of maximum iterations allowed. Defaults to 32.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        tolerance (float, optional): Convergence tolerance. Defaults to 1e-9.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        tuple[float, ...]: The converged value, or the branches of the solution.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> x <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0.0</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> math<span style=color:#f92672>.</span>isclose(x, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.0</span>):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> (<span style=color:#f92672>-</span><span style=color:#ae81ff>1.0</span>,)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> (float(<span style=color:#e6db74>&#34;nan&#34;</span>),)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>elif</span> math<span style=color:#f92672>.</span>isclose(x, <span style=color:#ae81ff>0.0</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> (<span style=color:#ae81ff>0.0</span>, <span style=color:#ae81ff>1.0</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>elif</span> math<span style=color:#f92672>.</span>isclose(x, <span style=color:#ae81ff>1.0</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> (<span style=color:#ae81ff>1.0</span>,)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>elif</span> x <span style=color:#f92672>&lt;</span> BRANCH_POINT:
</span></span><span style=display:flex><span>        z1 <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>            <span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> (math<span style=color:#f92672>.</span>e <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>*</span> math<span style=color:#f92672>.</span>exp(math<span style=color:#f92672>.</span>e <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.5</span>) <span style=color:#f92672>*</span> math<span style=color:#f92672>.</span>sqrt(math<span style=color:#f92672>.</span>exp(<span style=color:#f92672>-</span>math<span style=color:#f92672>.</span>e) <span style=color:#f92672>-</span> x)
</span></span><span style=display:flex><span>        ) <span style=color:#f92672>/</span> math<span style=color:#f92672>.</span>e
</span></span><span style=display:flex><span>        z2 <span style=color:#f92672>=</span> math<span style=color:#f92672>.</span>exp(<span style=color:#ae81ff>0.15</span> <span style=color:#f92672>*</span> math<span style=color:#f92672>.</span>e <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>*</span> x<span style=color:#f92672>**</span><span style=color:#ae81ff>0.15</span>
</span></span><span style=display:flex><span>        z3 <span style=color:#f92672>=</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> math<span style=color:#f92672>.</span>sqrt(<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> math<span style=color:#f92672>.</span>exp(<span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> math<span style=color:#f92672>.</span>e) <span style=color:#f92672>*</span> x <span style=color:#f92672>*</span> x)) <span style=color:#f92672>/</span> math<span style=color:#f92672>.</span>e
</span></span><span style=display:flex><span>        L <span style=color:#f92672>=</span> math<span style=color:#f92672>.</span>log(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Flags to check for convergence</span>
</span></span><span style=display:flex><span>        c1 <span style=color:#f92672>=</span> c2 <span style=color:#f92672>=</span> c3 <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># These will be L * x^z for each branch.</span>
</span></span><span style=display:flex><span>        Lxz1 <span style=color:#f92672>=</span> Lxz2 <span style=color:#f92672>=</span> Lxz3 <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Iterate for x^x^y = y until convergence</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(MAX_ITER):
</span></span><span style=display:flex><span>            <span style=color:#75715e># Keeping track of previous z&#39;s to check for convergence</span>
</span></span><span style=display:flex><span>            zp1, zp2, zp3 <span style=color:#f92672>=</span> z1, z2, z3
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> c1:
</span></span><span style=display:flex><span>                Lxz1 <span style=color:#f92672>=</span> L <span style=color:#f92672>*</span> x<span style=color:#f92672>**</span>z1
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> c2:
</span></span><span style=display:flex><span>                Lxz2 <span style=color:#f92672>=</span> L <span style=color:#f92672>*</span> x<span style=color:#f92672>**</span>z2
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> c3:
</span></span><span style=display:flex><span>                Lxz3 <span style=color:#f92672>=</span> L <span style=color:#f92672>*</span> x<span style=color:#f92672>**</span>z3
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># These cases should NEVER occur. It is here just in case.</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> z1 <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;WARNING: iteration </span><span style=color:#e6db74>{</span>i <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74> encountered negative z1: </span><span style=color:#e6db74>{</span>z1<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                z1 <span style=color:#f92672>=</span> x<span style=color:#f92672>**</span>z1
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> z2 <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;WARNING: iteration </span><span style=color:#e6db74>{</span>i <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74> encountered negative z1: </span><span style=color:#e6db74>{</span>z2<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                z2 <span style=color:#f92672>=</span> x<span style=color:#f92672>**</span>z2
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> z3 <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;WARNING: iteration </span><span style=color:#e6db74>{</span>i <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74> encountered negative z1: </span><span style=color:#e6db74>{</span>z3<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                z3 <span style=color:#f92672>=</span> x<span style=color:#f92672>**</span>z3
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>                <span style=color:#75715e># Newton-Raphson iterations</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> c1:
</span></span><span style=display:flex><span>                    z1 <span style=color:#f92672>*=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> (math<span style=color:#f92672>.</span>log(z1) <span style=color:#f92672>-</span> Lxz1) <span style=color:#f92672>/</span> (z1 <span style=color:#f92672>*</span> L <span style=color:#f92672>*</span> Lxz1 <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> c2:
</span></span><span style=display:flex><span>                    z2 <span style=color:#f92672>*=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> (math<span style=color:#f92672>.</span>log(z2) <span style=color:#f92672>-</span> Lxz2) <span style=color:#f92672>/</span> (z2 <span style=color:#f92672>*</span> L <span style=color:#f92672>*</span> Lxz2 <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> c3:
</span></span><span style=display:flex><span>                    z3 <span style=color:#f92672>*=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> (math<span style=color:#f92672>.</span>log(z3) <span style=color:#f92672>-</span> Lxz3) <span style=color:#f92672>/</span> (z3 <span style=color:#f92672>*</span> L <span style=color:#f92672>*</span> Lxz3 <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>ValueError</span> <span style=color:#66d9ef>as</span> Err:
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Iteration </span><span style=color:#e6db74>{</span>i <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74> failed with error: </span><span style=color:#e6db74>{</span>Err<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;z1 = </span><span style=color:#e6db74>{</span>z1<span style=color:#e6db74>}</span><span style=color:#e6db74>, Lxz1 = </span><span style=color:#e6db74>{</span>Lxz1<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;z1 = </span><span style=color:#e6db74>{</span>z2<span style=color:#e6db74>}</span><span style=color:#e6db74>, Lxz1 = </span><span style=color:#e6db74>{</span>Lxz2<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;z1 = </span><span style=color:#e6db74>{</span>z3<span style=color:#e6db74>}</span><span style=color:#e6db74>, Lxz1 = </span><span style=color:#e6db74>{</span>Lxz3<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>return</span> (z1, z2, z3)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>ZeroDivisionError</span> <span style=color:#66d9ef>as</span> Err:
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Iteration </span><span style=color:#e6db74>{</span>i <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74> failed with error: </span><span style=color:#e6db74>{</span>Err<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;z1 = </span><span style=color:#e6db74>{</span>z1<span style=color:#e6db74>}</span><span style=color:#e6db74>, Lxz1 = </span><span style=color:#e6db74>{</span>Lxz1<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;z1 = </span><span style=color:#e6db74>{</span>z2<span style=color:#e6db74>}</span><span style=color:#e6db74>, Lxz1 = </span><span style=color:#e6db74>{</span>Lxz2<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;z1 = </span><span style=color:#e6db74>{</span>z3<span style=color:#e6db74>}</span><span style=color:#e6db74>, Lxz1 = </span><span style=color:#e6db74>{</span>Lxz3<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>return</span> (z1, z2, z3)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># Check for convergence</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> c1:
</span></span><span style=display:flex><span>                c1 <span style=color:#f92672>=</span> abs(z1 <span style=color:#f92672>-</span> zp1) <span style=color:#f92672>&lt;</span> tolerance <span style=color:#f92672>*</span> max(abs(z1), <span style=color:#ae81ff>1.0</span>)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> c2:
</span></span><span style=display:flex><span>                c2 <span style=color:#f92672>=</span> abs(z2 <span style=color:#f92672>-</span> zp2) <span style=color:#f92672>&lt;</span> tolerance <span style=color:#f92672>*</span> max(abs(z2), <span style=color:#ae81ff>1.0</span>)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> c3:
</span></span><span style=display:flex><span>                c3 <span style=color:#f92672>=</span> abs(z3 <span style=color:#f92672>-</span> zp3) <span style=color:#f92672>&lt;</span> tolerance <span style=color:#f92672>*</span> max(abs(z3), <span style=color:#ae81ff>1.0</span>)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> c1 <span style=color:#f92672>and</span> c2 <span style=color:#f92672>and</span> c3:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> verbose:
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Converged in </span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74> iterations.&#34;</span>)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>return</span> (z1, z2, z3)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        print(
</span></span><span style=display:flex><span>            <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;WARNING: iteration </span><span style=color:#e6db74>{</span>i <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74> failed to converge after </span><span style=color:#e6db74>{</span>MAX_ITER<span style=color:#e6db74>}</span><span style=color:#e6db74> iterations.&#34;</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;x: </span><span style=color:#e6db74>{</span>x<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> (z1, z2, z3)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>elif</span> x <span style=color:#f92672>&lt;</span> MAX_VALUE:
</span></span><span style=display:flex><span>        z <span style=color:#f92672>=</span> ((<span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>2.051</span> <span style=color:#f92672>*</span> x) <span style=color:#f92672>*</span> x <span style=color:#f92672>-</span> <span style=color:#ae81ff>6.037</span>) <span style=color:#f92672>/</span> ((<span style=color:#ae81ff>16.183</span> <span style=color:#f92672>-</span> <span style=color:#ae81ff>4.006</span> <span style=color:#f92672>*</span> x) <span style=color:#f92672>*</span> x <span style=color:#f92672>-</span> <span style=color:#ae81ff>15.134</span>)
</span></span><span style=display:flex><span>        L <span style=color:#f92672>=</span> math<span style=color:#f92672>.</span>log(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Iterate for x^y = y until convergence</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(MAX_ITER):
</span></span><span style=display:flex><span>            zp <span style=color:#f92672>=</span> z  <span style=color:#75715e># previous z</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> z <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                <span style=color:#75715e># This should never occur. It is here just in case.</span>
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;WARNING: iteration </span><span style=color:#e6db74>{</span>i <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74> encountered negative z: </span><span style=color:#e6db74>{</span>z<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                z <span style=color:#f92672>=</span> x<span style=color:#f92672>**</span>z
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>                <span style=color:#75715e># Newton-Raphson iteration</span>
</span></span><span style=display:flex><span>                z <span style=color:#f92672>*=</span> (math<span style=color:#f92672>.</span>log(z) <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>/</span> (L <span style=color:#f92672>*</span> z <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>ValueError</span> <span style=color:#66d9ef>as</span> Err:
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Iteration </span><span style=color:#e6db74>{</span>i <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74> failed with ValueError: </span><span style=color:#e6db74>{</span>Err<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;z = </span><span style=color:#e6db74>{</span>z<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>return</span> (z,)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>ZeroDivisionError</span> <span style=color:#66d9ef>as</span> Err:
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Iteration </span><span style=color:#e6db74>{</span>i <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74> failed with ZeroDivisionError: </span><span style=color:#e6db74>{</span>Err<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;z = </span><span style=color:#e6db74>{</span>z<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>return</span> (z,)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> abs(z <span style=color:#f92672>-</span> zp) <span style=color:#f92672>&lt;</span> tolerance <span style=color:#f92672>*</span> max(abs(z), <span style=color:#ae81ff>1.0</span>):
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> verbose:
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Converged in </span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74> iterations.&#34;</span>)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>return</span> (z,)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        print(
</span></span><span style=display:flex><span>            <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;WARNING: iteration </span><span style=color:#e6db74>{</span>i <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74> failed to converge after </span><span style=color:#e6db74>{</span>MAX_ITER<span style=color:#e6db74>}</span><span style=color:#e6db74> iterations.&#34;</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;x: </span><span style=color:#e6db74>{</span>x<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> (z,)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>elif</span> math<span style=color:#f92672>.</span>isclose(x, MAX_VALUE):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> (math<span style=color:#f92672>.</span>e,)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> (float(<span style=color:#e6db74>&#34;inf&#34;</span>),)
</span></span></code></pre></div><p>Potential improvements to my code could be to vectorize computations for the branched section of the curve using NumPy, using <a href=https://docs.python.org/3/library/math.html#math.fma>math.fma()</a> to take advantage of multiplier–accumulators in modern CPUs, or just writing the code in a lower-level language like C++ or Rust instead. But the code is already blazingly fast for me, so I am satisfied with the approach.</p><p>Thanks for reading!</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://blog.mileswaugh.com/tags/math/>Math</a></li><li><a href=https://blog.mileswaugh.com/tags/numerical-analysis/>Numerical Analysis</a></li><li><a href=https://blog.mileswaugh.com/tags/long/>Long</a></li></ul><nav class=paginav><a class=next href=https://blog.mileswaugh.com/posts/complex_fractals_desmos/><span class=title>Next »</span><br><span>Complex Fractals in Desmos</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Efficiently Computing Infinite Tetration on x" href="https://x.com/intent/tweet/?text=Efficiently%20Computing%20Infinite%20Tetration&amp;url=https%3a%2f%2fblog.mileswaugh.com%2fposts%2finfinite_tetration%2f&amp;hashtags=math%2cnumericalanalysis%2clong"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Efficiently Computing Infinite Tetration on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fblog.mileswaugh.com%2fposts%2finfinite_tetration%2f&amp;title=Efficiently%20Computing%20Infinite%20Tetration&amp;summary=Efficiently%20Computing%20Infinite%20Tetration&amp;source=https%3a%2f%2fblog.mileswaugh.com%2fposts%2finfinite_tetration%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Efficiently Computing Infinite Tetration on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fblog.mileswaugh.com%2fposts%2finfinite_tetration%2f&title=Efficiently%20Computing%20Infinite%20Tetration"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Efficiently Computing Infinite Tetration on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fblog.mileswaugh.com%2fposts%2finfinite_tetration%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Efficiently Computing Infinite Tetration on whatsapp" href="https://api.whatsapp.com/send?text=Efficiently%20Computing%20Infinite%20Tetration%20-%20https%3a%2f%2fblog.mileswaugh.com%2fposts%2finfinite_tetration%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Efficiently Computing Infinite Tetration on telegram" href="https://telegram.me/share/url?text=Efficiently%20Computing%20Infinite%20Tetration&amp;url=https%3a%2f%2fblog.mileswaugh.com%2fposts%2finfinite_tetration%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Efficiently Computing Infinite Tetration on ycombinator" href="https://news.ycombinator.com/submitlink?t=Efficiently%20Computing%20Infinite%20Tetration&u=https%3a%2f%2fblog.mileswaugh.com%2fposts%2finfinite_tetration%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=piano-miles/blog data-repo-id=R_kgDOJweFuQ data-category=Announcements data-category-id=DIC_kwDOJweFuc4CxvIv data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://blog.mileswaugh.com/>Waugh Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>